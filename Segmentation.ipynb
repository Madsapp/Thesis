{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.use('Qt5Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.signal import find_peaks, peak_widths, argrelmax\n",
    "from skimage.morphology import label, convex_hull_image\n",
    "from scipy import ndimage as ndi\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigger fontsize for x- and y-label\n",
    "plt.rc('xtick', labelsize = 12)\n",
    "plt.rc('ytick', labelsize = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_4x   = '/mnt/data/MAXIBONE/Goats/tomograms/hdf5-byte/scale/4x/'\n",
    "\n",
    "path_1x = '/mnt/data/MAXIBONE/Goats/tomograms/hdf5-byte/scale/1x/'\n",
    "\n",
    "seg_path  ='/mnt/data/MAXIBONE/Workspace/Mads/Concatenated_{scale}_samples/'.format(scale='1x')\n",
    "\n",
    "mask_path ='/mnt/data/MAXIBONE/Workspace/Mads/Bit_masks/'\n",
    "\n",
    "samples = ['769c.h5','770c.h5','771c.h5', '772.h5','774.h5','775.h5','778c.h5',\\\n",
    "          '780c.h5','785c.h5', '788c.h5','810c.h5','811.h5','815.h5','819.h5']\n",
    "\n",
    "samples_1x = ['769.h5','769c.h5','770.h5', '770c.h5','771.h5', '771c.h5', '772.h5',\\\n",
    "              '774.h5','775.h5', '775c.h5','778.h5', '778c.h5','780c.h5', '785c.h5',\\\n",
    "              '788c.h5','810.h5','810c.h5','811.h5','815.h5','819.h5']\n",
    "\n",
    "XX = np.linspace(1,255,254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show(x, n, cmap = None, vmax = None, vmin = None, dual_view = False, savefig = False):\n",
    "    \"\"\"Gives a quick view of an image, without writing too many lines\"\"\"\n",
    "    \"\"\"x: 2d-array to be projected; n: size of the imshow projection ; dual_view: If two images are to be shown simultaneuosly\"\"\"\n",
    "\n",
    "    if dual_view:\n",
    "        fig, ax = plt.subplots(ncols = 2, figsize = (n,n))\n",
    "        im =  ax[0].imshow(x[0])\n",
    "        im2 = ax[1].imshow(x[1])\n",
    "        if savefig:\n",
    "            fig.savefig('Fedt_dobbeltplot.eps')\n",
    "    else:\n",
    "        plt.figure(figsize = (n,n))\n",
    "        plt.imshow(x,cmap = cmap, vmax = vmax, vmin = vmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates indices round a circle with the given radius\n",
    "def circle(r, center):\n",
    "    \"\"\"Creates indices in a circular pattern\"\"\"\n",
    "    \"\"\"r: Radius of the circle pattern; center: center of the circle\"\"\"\n",
    "    T = int(2 * r * np.pi)\n",
    "    x, y = r * [0], r * [0]\n",
    "    for i,theta in enumerate(np.linspace(0,2*np.pi,r)):\n",
    "        x[i] = r * np.cos(theta)\n",
    "        y[i] = r * np.sin(theta)\n",
    "    return (np.array(x)+center).astype(int), (np.array(y)+center).astype(int)\n",
    "\n",
    "\n",
    "def rad_scan3d2(data):\n",
    "    \"\"\"Scan sample and creates histograms for each radial layer\"\"\"\n",
    "    \"\"\"data: 3d-array - CT data\"\"\"\n",
    "    rad = data.shape[1]//2\n",
    "    bins = np.arange(1,256)\n",
    "    xy = [circle(r, rad) for r in range(1,rad)]\n",
    "    arr = [np.histogram(np.concatenate((data[:,i[0],i[1]],data[:, i[0], -i[1] + rad*2])), bins = bins)[0] for i in xy]\n",
    "    return np.array(arr)\n",
    "\n",
    "\n",
    "\n",
    "def mask(data):\n",
    "    \"\"\"Mask for removing all values outside the cylinder \"\"\"\n",
    "    \"\"\"data: 3d-array - CT data\"\"\"\n",
    "    Ny, Nx = data.shape\n",
    "    R = Nx / 2\n",
    "    xs = np.linspace(-R,R,Nx)\n",
    "    ys = np.linspace(-R,R,Ny)\n",
    "    rs = np.sqrt(xs[None,:]**2 + ys[:,None]**2)\n",
    "    mask = rs <= R\n",
    "    return mask\n",
    "\n",
    "def mask_seg3D(data,r1,r2):\n",
    "    \"\"\"Mask for removing values between two radii\"\"\"\n",
    "    \"\"\"data: 3d-array - CT data; r1: outer layer radius; r2: inner layers radius\"\"\"\n",
    "    R = data.shape[1]/2 # the total radius of the CT\n",
    "    l = np.zeros(data.shape[0])# empty array used for broadcasting from 2d to 3d\n",
    "    xy = data.shape[1] # the diameter of the CT\n",
    "    c = np.linspace(-R,R,xy)# an array with distances from the middle with as many step as the diameter\n",
    "    rs = np.sqrt(c[None,:]**2 + c[:,None]**2)# broadcasting while calculating the euclidian distance from the center\n",
    "    outer_mask = rs <= r1 # exclude values outside the outer ring\n",
    "    inner_mask = rs >= r2 # exclude values inside the inner ring\n",
    "    mask = outer_mask == inner_mask # where the inner and outer ring overlaps\n",
    "    mask = l[:,None,None] + mask[None,:,:] # broadcasts the masked-array in 3d\n",
    "    return mask.astype(bool) * data\n",
    "\n",
    "\n",
    "# Scans sample along the z-, y- and x-axis\n",
    "def zyx_scan2(data):\n",
    "    \"\"\"Takes raw CT input and creates histograms in Z-,Y- and X-direction\"\"\"\n",
    "    \"\"\"data: 3d-array - CT data\"\"\"\n",
    "    masked = mask(data[0,:,:]) # uses the above mask function on the data \n",
    "    bins = np.arange(1,256)# bins used for histogram binning\n",
    "    z_ref = np.array([data[z] * masked for z in range(np.min(data.shape))])# applies the mask onto every layer\n",
    "    \n",
    "    #Creates thte 2d histograms for Z, Y and X with list comprehension\n",
    "    z_list = np.array([np.histogram(data[z] * masked,         bins = bins)[0] for z in range(data.shape[0])])\n",
    "    y_list = np.array([np.histogram(z_ref[:,y,:].reshape(-1), bins = bins)[0] for y in range(data.shape[1])])\n",
    "    x_list = np.array([np.histogram(z_ref[:,:,x].reshape(-1), bins = bins)[0] for x in range(data.shape[2])])\n",
    "    return z_list, y_list, x_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def implant_bitmask(data,threshold):\n",
    "    \"\"\"Creates a mask for implant and air for each layer, combining them to a full bit-maks\"\"\"\n",
    "    \"\"\"data: 3d-array - CT data; threshold: Intensity threshold to exclude all data below the given value \"\"\"\n",
    "    implant_mask = np.zeros_like(data, dtype = bool)# empty array for mask\n",
    "    implant_mask[data > threshold] = True # marks everything equal or above threshold as True\n",
    "    tot = np.prod(data.shape[1:])//100 # 1 percent of the total amount of voxels in one slice\n",
    "    for i in range(len(implant_mask)):# for loop for labeling clusters\n",
    "        lab = ndi.label(implant_mask[i])# labels clusters\n",
    "        test = np.where(lab[0] > 0)# checks how many voxels are above threshold \n",
    "        clusters = np.array([np.where(lab[0]==x)[0].size for x in range(1,5)])# caculate sizes of the labeled clusters\n",
    "        if len(test[0]) < tot:# if the number of voxels above threshold is less than 1 percent, then everything is excluded\n",
    "            implant_mask[i] = False\n",
    "        else:\n",
    "            max_cluster = clusters.argmax() + 1 # the biggest cluster that is not 0 i.e empty voxels below threshhold\n",
    "            max_indices = np.where(lab[0] == max_cluster)\n",
    "            y,x = max_indices\n",
    "            left =  y[x.argmin()] # max y val at min x val\n",
    "            right = y[x.argmax()] # max y val at max x val\n",
    "            # The implant is not always level in the horizontal axis, so this if statement find out which sides is higher\n",
    "            # to mark the areas above properly\n",
    "            # Then marks everything above as True and fill holes, leaving the final mask\n",
    "            if left >= right:\n",
    "                implant_mask[i, :right+2] = True\n",
    "                implant_mask[i, right:left+2, :x.min()] = True\n",
    "                implant_mask[i] = ndi.binary_fill_holes(implant_mask[i])\n",
    "            elif left <= right:\n",
    "                implant_mask[i, :left+2] = True\n",
    "                implant_mask[i, left:right+2, x.max():] = True\n",
    "                implant_mask[i] = ndi.binary_fill_holes(implant_mask[i])\n",
    "    return ~implant_mask # inverts mask in the end\n",
    "\n",
    "def top_mask_fixer(mask,idx):\n",
    "    \"\"\"Fixed error of the implant_bitmask\"\"\"\n",
    "    \"\"\"mask: mask to be applied; idx: indices of the mask\"\"\"\n",
    "    if idx.min() < 150:\n",
    "        for i in range(int(idx.min()),int(idx.max())):\n",
    "            if np.all(mask[i]):\n",
    "                mask[i] = False\n",
    "            else:\n",
    "                continue\n",
    "    return mask\n",
    "    \n",
    "def mask_multiplier(tup,idx):\n",
    "    \"\"\"Combined bit-masks, or returned 1 if no mask were given\"\"\"\n",
    "    \"\"\"tup: the tuple of masks or integer or float; idx: indices of the mask\"\"\"\n",
    "    if type(tup) == type((1,2,2)):\n",
    "        mask = top_mask_fixer(tup[0]['mask_voxels'][idx],idx)\n",
    "        for i in range(1,len(tup)):\n",
    "            mask *= tup[i]['mask_voxels'][idx]\n",
    "        return mask\n",
    "    elif type(tup) == type(1) or type(tup) == type(1.0):\n",
    "        return 1\n",
    "    else:\n",
    "        return top_mask_fixer(tup['mask_voxels'][idx], idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data and uses all above functions to create four distinct 2D-histograms w.r.t. x-,y,z- and radial axis\n",
    "# Includes masks for removing unwanted data \n",
    "def load_2D_data(data, chunk_size, rotate, masks):\n",
    "    \"\"\"Takes CT-data and creates 2d histograms for radial-, Z-, Y- and X-axis\"\"\"\n",
    "    \"\"\"data: 3d-array - CT data; chunk_size: number of layers to be scanned simultaneuously \"\"\"\n",
    "    \"\"\"rotate: True if samples needs to be rotated, else False; masks: the masks for excluding unnecessary data, integer if no mask\"\"\"\n",
    "    z_len  = data['voxels'].shape[0]# How many layers are in the Z-direction\n",
    "    y_len  = data['voxels'].shape[1]# How many layers are in the Y- and X-direction\n",
    "    rad_len = y_len//2 # the radius of the sample \n",
    "    hist_z =   np.zeros((z_len,       254))# histogram of the 4 different transformations\n",
    "    hist_y =   np.zeros((y_len,       254))\n",
    "    hist_x =   np.zeros((y_len,       254))\n",
    "    hist_rad = np.zeros((rad_len - 1, 254))\n",
    "\n",
    "    for i in tqdm(range(0,z_len, chunk_size)):\n",
    "        ni = min(chunk_size, z_len-i)#takes minimum of the chunk-size and the remaining layers to make sure all layer are included\n",
    "        \n",
    "        indices = np.arange(i, i+ni)# Indices of the chunk\n",
    "        sample_frac = data['voxels'][indices]# the chunk of the Ct is loaded\n",
    "        \n",
    "        if rotate:# if the sample needs to be rotated\n",
    "            sample_frac = np.rot90(sample_frac, 3, (1,2))\n",
    "        \n",
    "        \n",
    "        sample_frac = sample_frac * mask_multiplier(masks,indices)# add masks to the chunk\n",
    "        \n",
    "        R = rad_scan3d2(sample_frac)# Histogram for radial\n",
    "        Z,Y,X = zyx_scan2(sample_frac)# Histogram for Z,Y and X\n",
    "        \n",
    "        hist_rad += R# Adds histograms to respective array\n",
    "        hist_y   += Y\n",
    "        hist_x   += X\n",
    "        hist_z[indices]   += Z\n",
    "    print('Data has been loaded!')\n",
    "\n",
    "    return hist_rad, hist_z, hist_y, hist_x\n",
    "\n",
    "def load_2D_correc(data, chunk_size, rotate, masks):\n",
    "    \"\"\"Same as load_2D_data but only for radial\"\"\"\n",
    "    z_len  = data['voxels'].shape[0]\n",
    "    y_len  = data['voxels'].shape[1]\n",
    "    rad_len = y_len//2\n",
    "    hist_rad = np.zeros((rad_len - 1, 254))\n",
    "\n",
    "    for i in tqdm(range(0,z_len, chunk_size)):\n",
    "        ni = min(chunk_size, z_len-i)\n",
    "        \n",
    "        indices = np.arange(i, i+ni)\n",
    "        sample_frac = data['voxels'][indices]\n",
    "        \n",
    "        sample_frac = sample_frac * mask_multiplier(masks,indices)\n",
    "        \n",
    "        R = rad_scan3d2(sample_frac)\n",
    "        hist_rad += R\n",
    "    print('Data has been loaded!')\n",
    "\n",
    "    return hist_rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fits(hist, axis_name, sample_name, scale, mask):\n",
    "    \"\"\"Saves a fitted 2d histogram as a .npy with proper naming\"\"\"\n",
    "    \"\"\"hist: 2d histogram; axis_name: axis of the histogram i.e. ZYX or radial\"\"\"\n",
    "    \"\"\"sample_name: Name of the CT; scale: What scale the sample is used i.e. 1x,2x,4x,8x and 16x\"\"\"\n",
    "    if mask == 1: # if only one mask is applied\n",
    "        string = '_masked'\n",
    "        np.save(axis_name + '_fits_' + sample_name[:-3] +'_'+ scale + string, hist)\n",
    "    elif mask == 2:# two masks applied\n",
    "        string = '_double_masked'\n",
    "        np.save(axis_name + '_fits_' + sample_name[:-3] +'_'+ scale + string, hist)\n",
    "    elif mask == 0:# no mask applied\n",
    "        sn = sample_name\n",
    "        np.save(axis_name + '_fits_' + sn[:-3]+'_' + scale, hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vars(hist, axis_name, sample_name, scale, mask):\n",
    "    \"\"\"Saves the variables of the distributions in each layer as a .npy files\"\"\"\n",
    "    \"\"\"Input are the same as in save_fits\"\"\"\n",
    "    if mask==1:\n",
    "        string = '_masked'\n",
    "        np.save(axis_name + '_parameters_' + sample_name[:-3] +'_'+ scale + string, hist)\n",
    "    elif mask== 0:\n",
    "        sn = sample_name\n",
    "        np.save(axis_name + '_parameters_' + sn[:-3]+'_' + scale, hist)\n",
    "    elif mask==2:\n",
    "        string = '_double_masked'\n",
    "        np.save(axis_name + '_parameters_' + sample_name[:-3] +'_'+ scale + string, hist)\n",
    "        \n",
    "\n",
    "def load_processed_histograms_rzyx(sample,scale,mask):\n",
    "    \"\"\"Loads the histograms of a given sample\"\"\"\n",
    "    if mask ==1:\n",
    "        return np.load('rad_hist_'+ sample[:-3]+'_'+ scale +'_masked.npy', allow_pickle = True ),\\\n",
    "    np.load('z_hist_'+ sample[:-3]+'_'+ scale +'_masked.npy', allow_pickle = True ),\\\n",
    "    np.load('y_hist_'+ sample[:-3]+'_'+ scale +'_masked.npy', allow_pickle = True ),\\\n",
    "    np.load('x_hist_'+ sample[:-3]+'_'+ scale +'_masked.npy', allow_pickle = True )\n",
    "    elif mask==0:\n",
    "        return np.load('rad_hist_'+ sample[:-3]+'_'+ scale +'.npy', allow_pickle = True ),\\\n",
    "    np.load('z_hist_'+ sample[:-3]+'_'+ scale +'.npy', allow_pickle = True ),\\\n",
    "    np.load('y_hist_'+ sample[:-3]+'_'+ scale +'.npy', allow_pickle = True ),\\\n",
    "    np.load('x_hist_'+ sample[:-3]+'_'+ scale +'.npy', allow_pickle = True )\n",
    "    elif mask ==2:\n",
    "        return np.load('rad_hist_'+ sample[:-3]+'_'+ scale +'_double_masked.npy', allow_pickle = True ),\\\n",
    "    np.load('z_hist_'+ sample[:-3]+'_'+ scale +'_double_masked.npy', allow_pickle = True ),\\\n",
    "    np.load('y_hist_'+ sample[:-3]+'_'+ scale +'_double_masked.npy', allow_pickle = True ),\\\n",
    "    np.load('x_hist_'+ sample[:-3]+'_'+ scale +'_double_masked.npy', allow_pickle = True )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fits(sample, scale, path = None):\n",
    "    \"\"\"Function that loads the fitted histograms of a given sample\"\"\"\n",
    "    return np.load('rad_fits_'+ scale + '_' + sample[:-3] +'_double_masked.npy', allow_pickle = True ),\\\n",
    "    np.load('z_fits_'  + scale + '_' + sample[:-3] +'_double_masked.npy', allow_pickle = True ),\\\n",
    "    np.load('y_fits_'  + scale + '_' + sample[:-3] +'_double_masked.npy', allow_pickle = True ),\\\n",
    "    np.load('x_fits_'  + scale + '_' + sample[:-3] +'_double_masked.npy', allow_pickle = True )\n",
    "    \n",
    "def load_parameters(sample, scale, path = None):\n",
    "    \"\"\"Function that loads the fitted parameters of a given sample\"\"\"\n",
    "    return np.load('rad_parameters_'+ scale + '_' + sample[:-3] +'.npy', allow_pickle = True ),\\\n",
    "    np.load('z_parameters_'  + scale + '_' + sample[:-3] +'.npy', allow_pickle = True ),\\\n",
    "    np.load('y_parameters_'  + scale + '_' + sample[:-3] +'.npy', allow_pickle = True ),\\\n",
    "    np.load('x_parameters_'  + scale + '_' + sample[:-3] +'.npy', allow_pickle = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hist(hist, axis_name, sample_name,scale, mask):\n",
    "    \" Function that saves a histogram as .npy file\"\n",
    "    if mask == 1:# if one mask is applied\n",
    "        string = '_masked'\n",
    "        np.save(axis_name + '_hist_' + sample_name[:-3] +'_'+ scale + string, hist)\n",
    "    \n",
    "    elif mask == 2:# if two masks are applied \n",
    "        string = '_double_masked'\n",
    "        np.save(axis_name + '_hist_' + sample_name[:-3] +'_'+ scale + string, hist)\n",
    "        \n",
    "    elif mask== 0:# if no mask are applied\n",
    "        sn = sample_name\n",
    "        np.save(axis_name + '_hist_' + sn[:-3]+'_' + scale, hist)\n",
    "\n",
    "def save_labels(hist, axis_name, sample_name,scale, mask):\n",
    "    \"\"\"Fucntion that save the labels of the the fitted histograms\"\"\"\n",
    "    if mask == 1:\n",
    "        string = '_masked'\n",
    "        np.save(axis_name + '_labels_' + sample_name[:-3] +'_'+ scale + string, hist)\n",
    "    \n",
    "    elif mask == 2:\n",
    "        string = '_double_masked'\n",
    "        np.save(axis_name + '_labels_' + sample_name[:-3] +'_'+ scale + string, hist)\n",
    "        \n",
    "    elif mask== 0:\n",
    "        sn = sample_name\n",
    "        np.save(axis_name + '_labels_' + sn[:-3]+'_' + scale, hist)\n",
    "        \n",
    "def load_n_save_hists(sample, scale, masks, rotate, masked, concat):\n",
    "    \"\"\"Function that loads and saves the histograms\"\"\"\n",
    "    if concat:\n",
    "        path = '/mnt/data/MAXIBONE/Workspace/Mads/Concatenated_'+ scale +'_samples/' + scale + '_'+ sample[:-3]+'_concat.h5'\n",
    "    else:\n",
    "        path = '/mnt/data/MAXIBONE/Goats/tomograms/hdf5-byte/scale/'+ scale +'/' + sample\n",
    "    file = h5py.File(path, 'r')\n",
    "    n = file['voxels'].shape[0]//(10//int(scale[0]))\n",
    "    print(n)\n",
    "    R,Z,Y,X = load_2D_data(file, n, rotate, masks)\n",
    "    save_hist(R,'rad', str(sample), scale, masked)\n",
    "    save_hist(Y,'y'  , str(sample), scale, masked)\n",
    "    save_hist(X,'x'  , str(sample), scale, masked)\n",
    "    save_hist(Z,'z'  , str(sample), scale, masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fits_rzyx(fitted_distributions, variables, scale, sample, mask):\n",
    "    \"\"\"Saves all fitted distributions and their respective variables to a .npy file\"\"\"\n",
    "    R = fitted_distributions[0]\n",
    "    Z = fitted_distributions[1]\n",
    "    Y = fitted_distributions[2]\n",
    "    X = fitted_distributions[3]\n",
    "    R_vars = variables[0]\n",
    "    Z_vars = variables[1]\n",
    "    Y_vars = variables[2]\n",
    "    X_vars = variables[3]\n",
    "    if mask ==0:\n",
    "        string = ''\n",
    "    elif mask ==1:\n",
    "        string = '_masked'\n",
    "    elif mask ==2:\n",
    "        string = '_double_masked'\n",
    "        \n",
    "    np.save('rad_fits_' + scale  + '_' + sample[:-3] + string, R)\n",
    "    np.save('z_fits_'   + scale  + '_' + sample[:-3] + string, Z)\n",
    "    np.save('y_fits_'   + scale  + '_' + sample[:-3] + string, Y)\n",
    "    np.save('x_fits_'   + scale  + '_' + sample[:-3] + string, X)\n",
    "    \n",
    "    np.save('rad_parameters_' + scale + '_' + sample[:-3] + string, R_vars)\n",
    "    np.save('z_parameters_'   + scale + '_' + sample[:-3] + string, Z_vars)\n",
    "    np.save('y_parameters_'   + scale + '_' + sample[:-3] + string, Y_vars)\n",
    "    np.save('x_parameters_'   + scale + '_' + sample[:-3] + string, X_vars)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generel functions for fitting and describing data\n",
    "\n",
    "# Baseline distribution for all fits of histograms\n",
    "def power(vars,xs):\n",
    "    \"\"\"The function for the generalized gaussion distribution (GGD)\"\"\"\n",
    "    n = vars.size//4\n",
    "    abcd = np.reshape(vars,(4 ,n)).T\n",
    "    # broadcasts all sets of parameters in their own subsets so each distribution are separated\n",
    "    A, B, C, D = abcd[:,0,np.newaxis]**2, abcd[:,1,np.newaxis]**2, abcd[:,2,np.newaxis], abcd[:,3,np.newaxis]**2\n",
    "    X          = xs[np.newaxis,:]\n",
    "    d = 1 + np.sqrt(1 / (1 + D)) # Interval between 1 and 2\n",
    "    return np.sqrt(A) * np.exp(- np.sqrt(B) * np.power(np.abs(X - C), d ))\n",
    "\n",
    "\n",
    "def neg(f):\n",
    "    \"\"\"Penalty function to ensure fit doesn't go outside curves, a.e. so the distribution doesn't describe something there isn't there\"\"\"\n",
    "    neg = (np.abs(f) - f)/2\n",
    "    return neg\n",
    "\n",
    "\n",
    "def error_power(vars,x_data,f_exact, overshoot_penalty):\n",
    "    \"\"\"The fucntion the minimizer evaluates the variables for the GGDs function\"\"\"\n",
    "    fopt = np.sum(power(vars,x_data), axis = 0)\n",
    "    E    = np.sum((f_exact - fopt)**2)  + overshoot_penalty * np.sum(neg(f_exact - fopt)**2)\n",
    "    return E\n",
    "\n",
    "def sigma_spread(x,y,peaks):\n",
    "    \"\"\"Reverts the histogram to individual data points to find intial spread for the distributions\"\"\"\n",
    "    left = np.floor(peaks[1]['left_ips']).astype(int)# left side of the peaks found\n",
    "    right = np.ceil(peaks[1]['right_ips']).astype(int) # right side of the peaks found\n",
    "    ips = np.vstack((left,right)).T # stack left and right tails into single array\n",
    "    ips_expanded = [np.arange(i.min(),i.max()+1, 1) for i in ips] # indices in between left and right tails\n",
    "    values = [np.repeat(x[j],y[j].astype(int)) for j in ips_expanded] # the values given by the axis\n",
    "    spread = np.array([np.std(values[k]) for k in range(len(values))]) # calculates the spread of each peak\n",
    "    if np.any(spread==0):\n",
    "        idx = np.where(spread==0)[0]\n",
    "        spread[idx] = 0.1\n",
    "        return spread\n",
    "    else:\n",
    "        return 1 / (2*spread)**2\n",
    "\n",
    "\n",
    "#def normalise(x):\n",
    "#    \"\"\"normalises wrt maximum values in the slice\"\"\"\n",
    "#    if (np.max(x)-np.min(x)) == 0:\n",
    "#        return np.zeros_like(x)\n",
    "#    else:\n",
    "#        return(x-np.min(x))/(np.max(x)-np.min(x))\n",
    "    \n",
    "def normalise(x):\n",
    "    \"\"\"normalises distributions wrt to number of voxels pr slice\"\"\"\n",
    "    if (np.max(x)-np.min(x)) == 0:\n",
    "        return np.zeros_like(x)\n",
    "    else:\n",
    "        return x/x.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_spread2(x,y):\n",
    "    \"\"\"Function that avoids error while finding the spread\"\"\"\n",
    "    values = np.repeat(x,y.astype(int))\n",
    "    spread = np.std(values)\n",
    "    if np.isnan(spread): # if spread is zero\n",
    "        return 0 # to avoid True-divide\n",
    "    else:\n",
    "        return spread\n",
    "    \n",
    "def error_layer(distr):\n",
    "    \"\"\"find the standard deviation of each distribution in a class\"\"\"\n",
    "    error_arr = np.zeros((distr.shape[0],distr.shape[1]))\n",
    "    for i in range(len(error_arr)):\n",
    "        error_arr[i] += np.array([sigma_spread2(XX,distr[i][x]) for x in range(len(new_fit))])\n",
    "    return error_arr\n",
    "\n",
    "def convert_d(vars):\n",
    "    \"\"\"Reconvert d values back to actual values for saving variables\"\"\"\n",
    "    vars[-1] = 1 + np.sqrt(1 / (1 + vars[-1]))\n",
    "    return vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgraded minimizer\n",
    "def bounds(x0):\n",
    "    \"\"\"Make bounds for the initial guess for the distribution before being minimized\"\"\"\n",
    "    n = x0.size//4\n",
    "    A = np.array([(i/2,i) for i in x0[:n]])\n",
    "    B = np.array([(0.2,5) for i in range(n)])\n",
    "    C = np.array([(i-1,i+1) for i in x0[2*n:3*n]])\n",
    "    D = np.array([(0,1000) for i in range(n)])\n",
    "    return np.concatenate((A,B,C,D))\n",
    "\n",
    "def peak_info_yseg(data):\n",
    "    \"\"\"Function that find peak positions\"\"\"\n",
    "    initial_peaks = find_peaks(data)[0]# locate all \"peaks\" \n",
    "\n",
    "    PW,PH,l_ips,r_ips = peak_widths(data, initial_peaks)# calculate the peak tails\n",
    "    MH = np.mean(PH)/5# height threshold for accepting peaks as actual peaks\n",
    "    final = np.array([x for x in range(len(PW)) if PH[x] >= MH]).astype(int) # Filters actual peaks from \"peaks\"\n",
    "    dict  = {'left_ips': l_ips[final],'right_ips': r_ips[final]}\n",
    "    return initial_peaks[final], dict\n",
    "\n",
    "def initial_trial6(xs,data, method = 'power'):\n",
    "    \"\"\" returns the initial guess values for the minimizer \"\"\"\n",
    "    init_peaks = peak_info_yseg(data)\n",
    "\n",
    "    peak_idx = init_peaks[0]\n",
    "    n = len(peak_idx) # number of peaks\n",
    "    A = data[peak_idx] # Amplitude of the distribution\n",
    "    C = xs[peak_idx] # Mean/mode of distribution\n",
    "    B = sigma_spread(xs,data,init_peaks) # the spread of each distribution\n",
    "    if method == 'gauss':\n",
    "        vars = np.concatenate((A,B,C))\n",
    "    elif method == 'power':\n",
    "        D = np.ones(n)*0.5\n",
    "        vars = np.concatenate((A,B,C,D))\n",
    "    return vars\n",
    "\n",
    "\n",
    "def minimizer_mask(xs, data, penalty):\n",
    "    \"\"\"return minimized-parameters and -distributions\"\"\"\n",
    "    vars = initial_trial6(xs, data)\n",
    "    \n",
    "    if len(vars) < 1: # return zeros if no peaksare registered\n",
    "        return np.zeros(1), np.zeros(len(xs))\n",
    "    else:# creates bounds and minimizes, if peaks are registered\n",
    "        bnds = bounds(vars)\n",
    "        fit = minimize(error_power,vars, args = (xs, data, penalty), bounds = bnds)\n",
    "        return fit.x, power(fit.x, xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_seg3Deux(data,r1,r2):\n",
    "    \"\"\"return mask in 3d, repeat function\"\"\"\n",
    "    R = data.shape[1]/2\n",
    "    l = np.zeros(data.shape[0])\n",
    "    xy = data.shape[1]\n",
    "    c = np.linspace(-R,R,xy)\n",
    "    rs = np.sqrt(c[None,:]**2 + c[:,None]**2)\n",
    "    outer_mask = rs <= r1\n",
    "    inner_mask = rs >= r2\n",
    "    mask = outer_mask == inner_mask\n",
    "    if data.shape == 3:\n",
    "        mask = l[:,None,None] + mask[None,:,:]\n",
    "        return np.where(mask==True)\n",
    "    else:\n",
    "        return np.where(mask==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_loc(image,clus_frac, plot):\n",
    "    \"\"\"Function that filter off clusters below a treshhold\"\"\"\n",
    "    \"\"\"image: 2d array; clus_frac: percentage threshold; plot: return imshow it True \"\"\"\n",
    "    counts = np.bincount(image.flatten()) # counts the number squares in each cluster\n",
    "    cluster_size = np.sum(counts[1:]) * (clus_frac/100) # To exclude clusters containing less than 1 % of the segmented data\n",
    "    rang = np.where(counts > cluster_size)[0] # list of clusters bigger than desired cluster size\n",
    "    list = []\n",
    "    for i in range(1,len(rang)): # not sure if this is necessary ?\n",
    "        picf = np.copy(image)\n",
    "        picf[np.where(picf != rang[i])] = 0 # converts values indifferent to cluster integer to 0, so only that cluster is extracted\n",
    "        list.append(picf//rang[i]) # Appends single cluster to list\n",
    "        if plot:\n",
    "            plt.figure(figsize  = (10,10))\n",
    "            plt.imshow(picf)\n",
    "    arr = np.asarray(list, dtype = int)\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] *=(i+1)\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_2_arr(list):\n",
    "    \"\"\"Convert the python lists with distributions into single 2d-array\"\"\"\n",
    "    y = len(list)\n",
    "    x = list[0].shape[-1]\n",
    "    array = np.zeros((y,x))\n",
    "    for i in range(y):\n",
    "        for j in range(len(list[i])):\n",
    "            array[i,:] += list[i][j]\n",
    "    return array\n",
    "\n",
    "def class_tracker(hist,vars):\n",
    "    \"\"\"Creates binary array with 1 on positions of mean/mode of peaks\"\"\"\n",
    "    arr = np.zeros_like(hist, dtype = 'float32')\n",
    "    for i in range(len(arr)):\n",
    "        n = vars[i].size//4\n",
    "        cs = vars[i][2*n:3*n]\n",
    "        cs = cs.astype(int)\n",
    "        arr[i,cs] += 1\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rad_fitter(data, penalty):\n",
    "    \"\"\"Returns list of parameters and distributions for radial\"\"\"\n",
    "    hist_rad = data\n",
    "    rad_list = []\n",
    "    rad_vars = []\n",
    "    for i in tqdm(range(len(hist_rad))):\n",
    "        fee = minimizer_mask(XX,hist_rad[i],penalty)\n",
    "        rad_vars.append(fee[0])\n",
    "        rad_list.append(fee[1])\n",
    "        #if i%(len(hist_rad)//10) ==0:\n",
    "            #print(np.round((100*i)/len(hist_rad)),' %  done')\n",
    "    print('The radial-histogram has been fitted')\n",
    "    return rad_list, rad_vars\n",
    "\n",
    "\n",
    "def y_fitter(data, penalty):\n",
    "    \"\"\"Returns list of parameters and distributions for Y\"\"\"\n",
    "    hist_y = data\n",
    "    y_list = []\n",
    "    y_vars = []\n",
    "    for i in tqdm(range(len(hist_y))):\n",
    "        fee = minimizer_mask(XX,hist_y[i],penalty)\n",
    "        y_vars.append(fee[0])\n",
    "        y_list.append(fee[1])\n",
    "        #if i%(len(hist_y)//10) ==0:\n",
    "            #print(np.round((100*i)//len(hist_y)), ' %  done')\n",
    "\n",
    "    print('The y-histogram has been fitted')\n",
    "    return y_list, y_vars\n",
    "\n",
    "def z_fitter(data, penalty):\n",
    "    \"\"\"Returns list of parameters and distributions for Z\"\"\"\n",
    "    hist_z = data\n",
    "    z_list = []\n",
    "    z_vars = []\n",
    "    for i in tqdm(range(len(hist_z))):\n",
    "        fee = minimizer_mask(XX,hist_z[i],penalty)\n",
    "        z_vars.append(fee[0])\n",
    "        z_list.append(fee[1])\n",
    "        #if i%(len(hist_z)//10) ==0:\n",
    "            #print(np.round((100*i)/len(hist_z)),' %  done')\n",
    "    print('The z-histogram has been fitted')\n",
    "    return z_list, z_vars\n",
    "\n",
    "def x_fitter(data, penalty):\n",
    "    \"\"\"Returns list of parameters and distributions for X\"\"\"\n",
    "    hist_x = data\n",
    "    print(len(data))\n",
    "    x_list = []\n",
    "    x_vars = []\n",
    "    for i in tqdm(range(len(hist_x))):\n",
    "        fee = minimizer_mask(XX,hist_x[i],1)\n",
    "        x_vars.append(fee[0])\n",
    "        x_list.append(fee[1])\n",
    "        #if i%(len(hist_x)//10) ==0:\n",
    "            #print(np.round((100*i)/len(hist_x)), ' %  done')\n",
    "    print('The x-histogram has been fitted')\n",
    "    return x_list, x_vars\n",
    "\n",
    "def combined_autofitter(data, penalty):\n",
    "    \"\"\"combined Fitting function for radial-, Z-,Y-,X-histograms\"\"\"\n",
    "    r_fit, r_var = rad_fitter(data[0], penalty)\n",
    "    z_fit, z_var = z_fitter  (data[1], penalty)\n",
    "    y_fit, y_var = y_fitter  (data[2], penalty)\n",
    "    x_fit, x_var = x_fitter  (data[3], penalty)\n",
    "    rad = [r_fit, r_var]\n",
    "    z =   [z_fit, z_var]\n",
    "    y =   [y_fit, y_var]\n",
    "    x =   [x_fit, x_var]\n",
    "    return rad, z, y, x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some peaks were underdescribed, these functions tried to compensate for that \n",
    "def insert_missing_peaks(data,params,indices):\n",
    "    \"\"\"update the parameters by adding more to each array\"\"\"\n",
    "    n = params.size//4\n",
    "    m = len(indices)\n",
    "    a = data[indices]\n",
    "    c = indices\n",
    "    b = np.ones_like(indices)\n",
    "    d = b*0.5\n",
    "    new_values = np.concatenate((a,b,c,d))\n",
    "    place = np.concatenate((m*[0],m*[n],m*[2*n], m*[3*n]))\n",
    "    new_params = np.insert(params,place, new_values)\n",
    "    return new_params\n",
    "\n",
    "def find_missing_peaks(data,fit):\n",
    "    \"\"\"Located all peaks to insert \"\"\"\n",
    "    mp_list = []\n",
    "    fit = list_2_arr(fit)\n",
    "    diff = data - fit\n",
    "    for i in range(len(fit)):\n",
    "        dib = argrelmax(diff[i])[0]\n",
    "        id1 = dib > 44\n",
    "        id2 = dib < 75\n",
    "        ID = id1==id2\n",
    "        diffp = (diff[i]-fit[i])[dib[ID]]\n",
    "        if len(diffp[diffp>8000]) >=1 :\n",
    "            mp_list.append([i,dib[ID][diffp>8000]])\n",
    "    return np.array(mp_list).reshape(len(mp_list),2)\n",
    "\n",
    "def improve_fits(xs, data, fit, penalty):\n",
    "    \"\"\"Applies missing peaks to the 2d histogram\"\"\"\n",
    "    hidden_peaks = find_missing_peaks(data,fit[0])\n",
    "    closed_fit = np.copy(fit[0])\n",
    "    updated_params = np.copy(fit[1])\n",
    "    print(len(hidden_peaks))\n",
    "    for i,j in tqdm(hidden_peaks):\n",
    "        new_params = insert_missing_peaks(data[i],fit[1][i],j)\n",
    "        bnds = bounds(new_params)\n",
    "        n_fit = minimize(error_power,new_params, args = (xs, data[i], penalty), bounds = bnds)\n",
    "        updated_params[i] = new_params\n",
    "        closed_fit[i] = power(n_fit.x,XX)\n",
    "        #function better \n",
    "    return closed_fit, updated_params\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ridges(fits, k = 0, label_frac = 5 , e_iter = 1 , d_iter = 6, show = True):\n",
    "    \"\"\" Find ridges from the list of distributions\"\"\"\n",
    "    \"\"\"fits: list of distributions and parameters; k: number of ones in middle column of 3x3 kernel\"\"\"\n",
    "    \"\"\"label_frac: percentage threshhold; e_iter: number of erosions; d_iter: number of dilations; show: plot image if True\"\"\"\n",
    "    distr  = list_2_arr(fits[0])# 2d array from list of distributions\n",
    "    ridges = class_tracker(distr, fits[1])# binary array of all peak positions\n",
    "    kern = np.zeros((3,3), dtype = 'uint8')# 3x3 kernel\n",
    "    kern[k:3,1] += 1# add ones to middle column, k =0, fills entire colums, k=1 only 2/3 of column is filled\n",
    "    erode = cv2.erode (ridges,  kernel = kern, iterations = e_iter)#erodes\n",
    "    d2    = cv2.dilate(erode,   kernel = kern, iterations = d_iter)# dilates\n",
    "\n",
    "    lab = label(d2, connectivity = 2)# ndimage label: Labels clusters with integers\n",
    "    lab_loc = label_loc(lab, label_frac, False)# removes clusters below threshhold sizes\n",
    "    print(len(lab_loc),' labels')# prints the number of clusters/labels\n",
    "    if show:# if True, plots labels and fitted 2d hist next to each other\n",
    "        img_show((lab_loc.sum(axis=0),distr), 25, dual_view = True)\n",
    "    return lab_loc\n",
    "\n",
    "\n",
    "\n",
    "def distr_sorter(distr,labels):\n",
    "    \"\"\"Sorting distributions into classes\"\"\"\n",
    "    \"\"\"distr: list of distributions; labels: labels given from find_ridges function\"\"\"\n",
    "    Arr = np.zeros((len(labels),len(distr),distr[0].shape[-1]))\n",
    "    print('Shape of array:', Arr.shape)# print shape of label array. axis 0 is number of labels\n",
    "\n",
    "    for i in range(1,len(distr)-1):\n",
    "        idxy = np.where(labels[:,i]>0)# where in the 2d array labels are situated\n",
    "        if distr[i].sum() == 0.0 or distr[i].sum() == 0:# to avoid error \n",
    "            continue\n",
    "            \n",
    "        elif len(distr[i]) > 1 and len(idxy[1]) > 1:\n",
    "            args = distr[i].argmax(axis=1)# where peaks are situated\n",
    "            idxy = np.where(labels[:,i] > 0)# redo idxy for some reason\n",
    "            labs = [abs(idxy[1]-x).argmin() for x in args]# combares all peaks positions with labels to determine which class is closest in value\n",
    "            lad = idxy[0][labs]# indices for matching labels\n",
    "            for j in range(len(distr[i])):\n",
    "                Arr[lad[j],i,:] += distr[i][j]# appends distributions to matching label\n",
    "                \n",
    "        else:# if only one labels is in the layer\n",
    "            for j in range(distr[i].shape[0]):\n",
    "                #print(distr[i].shape[0])\n",
    "                Arr[idxy[0],i] += distr[i][j]# adds distribution to label\n",
    "    return Arr\n",
    "\n",
    "# Sorts parameters for the distributions into appropiate classes\n",
    "\n",
    "def parameter_sorter(params,labels):\n",
    "    \"\"\"Takes the lists of parameters from each layer, sorting them into class\"\"\"\n",
    "    \"\"\"If the class isn't present in the given layer, a 0 is placed instead\"\"\"\n",
    "    \"\"\"params: list of parameters from minimizer; labels: labels gained from find_ridgees function\"\"\"\n",
    "    print('Size of array',len(params),'- Number of labels',len(labels))# print size of array and number of labels\n",
    "\n",
    "    mad_list = []\n",
    "    for f in range(len(labels)):\n",
    "        mad_list.append([])\n",
    "\n",
    "    for i in range(len(params)):\n",
    "        N = len(params[i])//4\n",
    "        cs = params[i][2*N:3*N]# peak positions\n",
    "        idxy = np.where(labels[:,i] > 0)# where the labeled array assigns a class\n",
    "        if len(idxy[0]) < 1:\n",
    "            lad = None\n",
    "        else:\n",
    "            labs = [abs(idxy[1]-x).argmin() for x in cs] # Finds overlap between class and distribution\n",
    "            lad = idxy[0][labs] # applies indices to the overlapping distribution\n",
    "        empty_class = np.isin(np.arange(len(labels)),lad) # array for classes the overlap and doesn't\n",
    "        empty = np.arange(len(labels))[~empty_class]# indices for non overlap\n",
    "        full  = np.arange(len(labels))[empty_class] # indices for overlap\n",
    "\n",
    "        for k in empty:\n",
    "            mad_list[k].append(0)\n",
    "        for j in full:\n",
    "            idx = np.where(lad==j)[0]\n",
    "            mad_list[j].append([convert_d(params[i][y::N]) for y in idx])\n",
    "    return mad_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no longer used\n",
    "def combined_fit_improver(data,fits):\n",
    "    \"\"\"Combined the three funtions for improving fits\"\"\"\n",
    "    upd_r = improve_fits(XX,data[0], fits[0], 5)\n",
    "    print('rad_fits has been updated')\n",
    "    upd_z = improve_fits(XX,data[1], fits[1], 5)\n",
    "    print('z_fits has been updated')\n",
    "    upd_y = improve_fits(XX,data[2], fits[2], 5)\n",
    "    print('y_fits has been updated')\n",
    "    upd_x = improve_fits(XX,data[3], fits[3], 5)\n",
    "    print('x_fits has been updated')\n",
    "    return  upd_r, upd_z, upd_y, upd_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_radmask(data,rmin):\n",
    "    \"\"\"Creates a mask for each layer in the radial transformation\"\"\"\n",
    "    \"\"\"data: CT-data; rmin: number of layers from 0 to rmin acting as 'first' layer\"\"\"\n",
    "    inner_layer = mask_seg3Deux(data,rmin,0)# number of voxels scale with r, so the first layer are from 0:rmin\n",
    "    remaining_layers = np.array([mask_seg3Deux(data,r,r-1) for r in range(rmin,(data.shape[1]//2))])# creates masks for remaming layers\n",
    "    #one mask for each +1 increment of the radius\n",
    "    mask = [inner_layer]# list with inner layer indices \n",
    "    for i in range(len(remaining_layers)):# appends remaining indices to list\n",
    "        mask.append(remaining_layers[i])\n",
    "    return np.array(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_h5py_files(shape, axis, scale, sample, n_class):\n",
    "    \"\"\"create h5py files to compress new segmented CT's\"\"\"\n",
    "    \"\"\"axis:Z,Y,X or Radial; scale:1x,2x,4x... ; sample: .h5 CT-sample name; n_class: number of classes\"\"\"\n",
    "    output_files = [h5py.File(axis +'_segmentation_' + scale + '_' + sample[:-3] +'_'+ 'Class'+ str(x+1) + '.h5','w') for x in range(n_class)]\n",
    "    for i in range(n_class):\n",
    "        output_files[i].create_dataset(\"mask_voxels\", shape, dtype=np.float, compression=\"lzf\")\n",
    "    output_voxels = [output_files[v]['mask_voxels'] for v in range(n_class)]\n",
    "    return output_voxels, output_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation functions \n",
    "\n",
    "def zyx_test_segmentation(data, fit, zyx, depth):\n",
    "    \"\"\"Segmentation function for Z, Y and X\"\"\"\n",
    "    arrs = []\n",
    "    \n",
    "    if zyx == 0: # meaning Z-segmentation\n",
    "        for j in range(len(fit)):\n",
    "                arrs.append(np.array([normalise(fit[j][val+depth,data[val,:,:]]) for val in range(data.shape[zyx])]))\n",
    "    elif zyx == 1: # Y-Segmentation\n",
    "        for j in range(len(fit)):\n",
    "            arrs.append(np.array([normalise(fit[j][val, data[:,val,:]]) for val in range(data.shape[zyx])]))\n",
    "                \n",
    "    elif zyx == 2: # X-Segmentation\n",
    "        for j in range(len(fit)):\n",
    "            arrs.append(np.array([normalise(fit[j][val, data[:,:,val]]) for val in range(data.shape[zyx])]))\n",
    "                \n",
    "    return arrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation functions \n",
    "\n",
    "def zyx_stodder_segmentation(data, fit, zyx, depth):\n",
    "\n",
    "    arrs = []\n",
    "    \n",
    "    if zyx == 0: # meaning Z-segmentation\n",
    "        for j in range(len(fit)):\n",
    "            if len(data) %100==0:\n",
    "                arrs.append(np.array([normalise(fit[j][val+depth,data[val,:,:]]) for val in range(data.shape[zyx])]))\n",
    "            else:\n",
    "                arrs.append(np.array([normalise(fit[j][val+depth,data[val,:,:]]) for val in range(data.shape[zyx]-1)]))\n",
    "                \n",
    "    elif zyx == 1: # Y-Segmentation\n",
    "        for j in range(len(fit)):\n",
    "            arrs.append(np.array([normalise(fit[j][val, data[:,val,:]]) for val in range(data.shape[zyx])]))\n",
    "                \n",
    "    elif zyx == 2: # X-Segmentation\n",
    "        for j in range(len(fit)):\n",
    "            arrs.append(np.array([normalise(fit[j][val, data[:,:,val]]) for val in range(data.shape[zyx])]))\n",
    "                \n",
    "    return arrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resin_mask(sample, scale, z_fit, chunk_size, concat, rot):\n",
    "    \"\"\"Creates resin-mask\"\"\"\n",
    "    \"\"\"sample: .h5 file of the CT; scale: the scale of the data i.e 1x, 2x etc\"\"\"\n",
    "    \"\"\"z_fit: The distribution to segment with; chunk_size: size of CT chunks used for each iteration \"\"\"\n",
    "    \"\"\"concat: if the CT to be used has been volume matched or not; rot: if the sample are to be rotated\"\"\"\n",
    "    if concat:\n",
    "        tomogram     = h5py.File(seg_path + scale + '_' + sample[:-3] + '_concat.h5', 'r')\n",
    "    else:\n",
    "        tomogram     = h5py.File(path_1x + sample, 'r')\n",
    "    \n",
    "    implant_mask = h5py.File(mask_path + 'Implant-bitmask_'+ scale + '_' + sample, 'r')# implant bitmask to remove implant and air\n",
    "    fit = z_fit.reshape(1,z_fit.shape[0],z_fit.shape[1])# shape the arr, so it matches the required shape for the segmentation\n",
    "    \n",
    "    shape = implant_mask['mask_voxels'].shape# to get dimensions to know the number of iterations through the z-axis\n",
    "\n",
    "    \n",
    "    resin_file = h5py.File('Resin-bitmask_' + scale + '_' + sample,'w')# create the resin bit-mask file\n",
    "    resin_file.create_dataset(\"mask_voxels\", shape, dtype=np.bool, compression=\"lzf\")# create dataset to input data\n",
    "    output_voxels = resin_file['mask_voxels']# variable to input segmented data into\n",
    "\n",
    "    print('H5py-files are ready to be written')\n",
    "\n",
    "    for i in tqdm(range(0, shape[0], chunk_size)): #shape[zyx]\n",
    "        ni = min(chunk_size, shape[0]-i) #zyx determines which axis to be iterated through\n",
    "        if rot:# if the chunk are to be rotated\n",
    "            Layer = np.rot90(tomogram['voxels'][i:i+ni],3,(1,2))# loads chunk of the CT\n",
    "        else:\n",
    "            Layer = tomogram['voxels'][i:i+ni]# loads chunk of the CT\n",
    "            \n",
    "        masked_sample = Layer * implant_mask['mask_voxels'][i:i+ni]# applies mask to CT chunk\n",
    "        segments = zyx_test_segmentation(masked_sample, fit, 0, i)# segments the masked tomography\n",
    "        output_voxels[i:i+ni] = segments[0] > 0.01 # appends binary resin bit-mask to the h5py file\n",
    "    \n",
    "    # closes all the files used after loop \n",
    "    resin_file.close()\n",
    "    tomogram.close()\n",
    "    implant_mask.close()\n",
    "    print('Resin-mask is ready to be used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bone_mask_partial_close(sample, scale, z_fit, chunk_size):\n",
    "    \"\"\"Creates partially closed bone bitmask with binary_fill_holes\"\"\"\n",
    "    \"\"\"sample: .h5 file of the CT; scale: the scale of the data i.e 1x, 2x etc\"\"\"\n",
    "    \"\"\"z_fit: The distribution to segment with; chunk_size: size of CT chunks used for each iteration \"\"\"\n",
    "    tomogram     = h5py.File(seg_path + scale + '_' + sample[:-3]+'_concat.h5', 'r')# path to the h5py file to access data\n",
    "    implant_mask = h5py.File(mask_path + 'Implant-bitmask_'+ scale + '_' + sample, 'r')# path to the h5py file to access data\n",
    "    fit = z_fit.reshape(1,z_fit.shape[0],z_fit.shape[1])# shape the arr, so it matches the required shape for the segmentation\n",
    "    \n",
    "    shape = implant_mask['mask_voxels'].shape# to get dimensions to know the number of iterations through the z-axis\n",
    "    bone_file = h5py.File('Bone-bitmask_' + scale + '_' + sample,'w')# create the bone bit-mask file\n",
    "    bone_file.create_dataset(\"mask_voxels\", shape, dtype=np.bool, compression=\"lzf\")# create dataset to input data\n",
    "    output_voxels = bone_file['mask_voxels']# variable to input segmented data into\n",
    "\n",
    "    print('H5py-files are ready to be written')\n",
    "    \n",
    "    for i in tqdm(range(0, shape[0], chunk_size)): #shape[zyx]\n",
    "        ni = min(chunk_size, shape[0]-i)#takes minimum of the chunk-size and the remaining layers to make sure all layer are included\n",
    "        Layer = tomogram['voxels'][i:i+ni]# loads chunk of the CT\n",
    "        masked_sample = Layer * implant_mask['mask_voxels'][i:i+ni]# applies mask to CT chunk\n",
    "        segments = zyx_test_segmentation(masked_sample, fit, 0, i)# segments the masked tomography\n",
    "        \n",
    "        if ni %100==0:# if statement to avoid error in the last layer\n",
    "            segments = np.array([ndi.binary_fill_holes(segments[0][f]>0.01) for f in range(ni)])# close holes in the segmented image\n",
    "            output_voxels[i:i+ni] = segments # appends binary resin bit-mask to the h5py file\n",
    "        else:\n",
    "            segments = np.array([ndi.binary_fill_holes(segments[0][f]>0.01) for f in range(ni-1)])# close holes in the segmented image\n",
    "            output_voxels[i:i+ni-1] = segments# appends binary resin bit-mask to the h5py file\n",
    "    \n",
    "    \n",
    "    # closes all the files used after loop \n",
    "    bone_file.close()\n",
    "    tomogram.close()\n",
    "    implant_mask.close()\n",
    "    print('bone-mask is ready to be used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bone_mask_fullclose(sample, scale, z_fit, chunk_size):  \n",
    "    \"\"\"Creates fully closed bone bitmask with convex_hull_image\"\"\"\n",
    "    \"\"\"sample: .h5 file of the CT; scale: the scale of the data i.e 1x, 2x etc\"\"\"\n",
    "    \"\"\"z_fit: The distribution to segment with; chunk_size: size of CT chunks used for each iteration \"\"\"\n",
    "    tomogram     = h5py.File(seg_path  + scale + '_' + sample[:-3]+'_concat.h5', 'r')# path to the h5py file to access data\n",
    "    implant_mask = h5py.File(mask_path + 'Implant-bitmask_'+ scale + '_' + sample, 'r')# path to the h5py file to access data\n",
    "    fit = z_fit.reshape(1,z_fit.shape[0],z_fit.shape[1])# shape the arr, so it matches the required shape for the segmentation\n",
    "    \n",
    "    # quirk of implant bit mask would make it bigger or smaller\n",
    "    # The smallest are chosen to make sure the mathc in segmentation\n",
    "    shape = implant_mask['mask_voxels'].shape\n",
    "    shape2 = tomogram['voxels'].shape\n",
    "    if shape[0] <= shape2[0]:\n",
    "        shape = shape\n",
    "    else:\n",
    "        shape = shape2\n",
    "    \n",
    "    \n",
    "    bone_file = h5py.File('Bone-bitmask2_' + scale + '_' + sample,'w')# create the bone bit-mask file\n",
    "    bone_file.create_dataset(\"mask_voxels\", shape, dtype=np.bool, compression=\"lzf\")# create dataset to input data\n",
    "    output_voxels = bone_file['mask_voxels']# variable to input segmented data into\n",
    "\n",
    "    print('H5py-files are ready to be written')\n",
    "    \n",
    "    for i in tqdm(range(0, shape[0], chunk_size)): #shape[zyx]\n",
    "        ni = min(chunk_size, shape[0]-i)#takes minimum of the chunk-size and the remaining layers to make sure all layer are included\n",
    "        Layer = tomogram['voxels'][i:i+ni]# loads chunk of the CT\n",
    "        masked_sample = Layer * implant_mask['mask_voxels'][i:i+ni]# applies mask to CT chunk\n",
    "        #segments = zyx_test_segmentation(masked_sample, fit, 0, i)\n",
    "        #segments = np.array([convex_hull_image(segments[0][f]) for f in range(ni)])\n",
    "        #output_voxels[i:i+ni] = segments\n",
    "        segments = zyx_stodder_segmentation(masked_sample, fit, 0, i) # only used x idx is out of range\n",
    "        if ni %100==0:\n",
    "            segments = np.array([convex_hull_image(segments[0][f]) for f in range(ni)])# close holes in the segmented image\n",
    "            output_voxels[i:i+ni] = segments# appends binary resin bit-mask to the h5py file\n",
    "        else:\n",
    "            segments = np.array([convex_hull_image(segments[0][f]) for f in range(ni-1)])# closes holes in the segmented image\n",
    "            output_voxels[i:i+ni-1] = segments# appends binary resin bit-mask to the h5py file\n",
    "    \n",
    "    # closes all the files used after loop    \n",
    "    bone_file.close()\n",
    "    tomogram.close()\n",
    "    implant_mask.close()\n",
    "    print('Bone-mask is ready to be used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_zyx_segmentation(sample, scale, fit, zyx, chunk_size):\n",
    "    \"\"\"Segments CT with given probability distributions\"\"\"\n",
    "    \"\"\"Creates partially closed bone bitmask with binary_fill_holes\"\"\"\n",
    "    \"\"\"sample: .h5 file of the CT; scale: the scale of the data i.e 1x, 2x etc\"\"\"\n",
    "    \"\"\"fit: The distribution to segment with; chunk_size: size of CT chunks used for each iteration \"\"\"\n",
    "    \"\"\"zyx: 0, 1 or 2 relating to Z-,Y- or X-segmentation repsectively \"\"\"\n",
    "    \n",
    "    tomogram     = h5py.File(seg_path + scale + '_' + sample[:-3]+'_concat.h5', 'r')# path to the h5py file to access data\n",
    "    implant_mask = h5py.File(mask_path + 'Implant-bitmask_'+ scale + '_' + sample, 'r')# path to the h5py file to access data\n",
    "    bone_mask = h5py.File(mask_path + 'Bone-bitmask2_'+ scale + '_' + sample, 'r')# path to the h5py file to access data\n",
    "    print('Files has been loaded')\n",
    "    \n",
    "    n = len(fit)# number of class\n",
    "    \n",
    "    # quirk of implant bit mask would make it bigger or smaller\n",
    "    # The smallest are chosen to make sure the mathc in segmentation\n",
    "    shape = implant_mask['mask_voxels'].shape\n",
    "    shape2 = tomogram['voxels'].shape\n",
    "    if shape[0] <= shape2[0]:\n",
    "        shape = shape\n",
    "    else:\n",
    "        shape = shape2\n",
    "    \n",
    "    axis = choose_axis(zyx)# gives Z,Y or X as astring given the value of zyx\n",
    "    \n",
    "    output, files = create_h5py_files(shape, axis, scale, sample, n) # Creates h5py files\n",
    "    print('H5py-files are ready to be written')\n",
    "    if zyx == 0: # Z-segmentation\n",
    "        for i in tqdm(range(0, shape[0], chunk_size)):\n",
    "            ni = min(chunk_size, shape[0]-i)# takes minimum of the chunk-size and the remaining layers to make sure all layer are included\n",
    "            \n",
    "            Layer   = tomogram['voxels'][i:i+ni]# loads chunk of the CT\n",
    "            implant = implant_mask['mask_voxels'][i:i+ni]# loads chunk of the bit-mask\n",
    "            bone    = bone_mask['mask_voxels'][i:i+ni]# loads chunk of the bit-mask\n",
    "            masked_sample = Layer * implant * bone# applies implant mask to CT chunk\n",
    "            segments = zyx_test_segmentation(masked_sample, fit, zyx, i)# segments the masked tomography\n",
    "            for seg in range(n):\n",
    "                output[seg][i:i+ni] = segments[seg]# appends segmented data to compressed h5py file\n",
    "    \n",
    "    elif zyx == 1:# Y-segmentation\n",
    "        for i in tqdm(range(0, shape[0], chunk_size)):\n",
    "            ni = min(chunk_size, shape[0]-i) # takes minimum of the chunk-size and the remaining layers to make sure all layer are included\n",
    "            \n",
    "            Layer   = tomogram['voxels'][i:i+ni]# loads chunk of the CT\n",
    "            implant = implant_mask['mask_voxels'][i:i+ni]# loads chunk of the bit-mask\n",
    "            bone    = bone_mask['mask_voxels'][i:i+ni]# loads chunk of the bit-mask\n",
    "            masked_sample = Layer * implant * bone# applies implant mask to CT chunk\n",
    "            segments = zyx_test_segmentation(masked_sample, fit, zyx, 0)# segments the masked tomography\n",
    "            for seg in range(n):\n",
    "                output[seg][i:i+ni] = np.moveaxis(segments[seg],0,-2)# appends segmented data to compressed h5py file\n",
    "                \n",
    "    elif zyx == 2:# X-segmentation\n",
    "        for i in tqdm(range(0, shape[0], chunk_size)): #shape[zyx]\n",
    "            ni = min(chunk_size, shape[0]-i)# takes minimum of the chunk-size and the remaining layers to make sure all layer are included\n",
    "            \n",
    "            Layer   = tomogram['voxels'][i:i+ni]# loads chunk of the CT\n",
    "            implant = implant_mask['mask_voxels'][i:i+ni]# loads chunk of the bit-mask\n",
    "            bone    = bone_mask['mask_voxels'][i:i+ni]# loads chunk of the bit-mask\n",
    "            masked_sample = Layer * implant * bone# applies implant mask to CT chunk\n",
    "            segments = zyx_test_segmentation(masked_sample, fit, zyx, 0)# segments the masked tomography\n",
    "            for seg in range(n):\n",
    "                output[seg][i:i+ni] = np.moveaxis(segments[seg],0,-1) # appends segmented data to compressed h5py file   \n",
    "\n",
    "    for k in range(n): # closes the generated files again\n",
    "        files[k].close()\n",
    "    tomogram.close()\n",
    "    implant_mask.close()\n",
    "    #bone_mask.close()\n",
    "    print('Files has been closed')\n",
    "    print(axis.upper() + '-segmentation is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_axis(dim):\n",
    "    \"\"\"number of dim choose the string to name the segmentation\"\"\"\n",
    "    if dim == 0:\n",
    "        axis = 'z'\n",
    "    elif dim ==1:\n",
    "        axis = 'y'\n",
    "    elif dim ==2:\n",
    "        axis = 'x'\n",
    "    return axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rad_segmentation4(data, fit, m_ind, rmin):\n",
    "    \"\"\"Segments CT with given probability distributions, with the radial transformation\"\"\"\n",
    "    \"\"\"fit: The distribution to segment with; m_ind: mask indices; rmin: outer limit for the first chunk of layers \"\"\"\n",
    "    chunk = rmin# the chunk size of the inner layer\n",
    "    arrs = [np.zeros_like(data).astype(float) for i in range(len(fit))]\n",
    "    for i in range(len(fit[0])-chunk):# chunk is subtracted to adjust for the first layers taken in one go\n",
    "        if i == 0:# to include the inner chunk first\n",
    "            r1 = chunk\n",
    "            hist0 = fit[:][:r1].sum(axis=(0,1))\n",
    "            _,fit0 = minimizer_mask(XX,hist0,5)\n",
    "            for j in range(len(fit0)):# segments the inner chunk first\n",
    "                arrs[j][:,m_ind[i,0],m_ind[i,1]] += normalise(fit0[j,data[:,m_ind[i,0],m_ind[i,1]]])\n",
    "        else:\n",
    "            for j in range(len(fit)):\n",
    "                arrs[j][:,m_ind[i,0],m_ind[i,1]] += normalise(fit[j][i+chunk,data[:,m_ind[i,0],m_ind[i,1]]])\n",
    "    return arrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def final_rad_segmentation(sample, scale, fit, rmin, chunk_size):\n",
    "    \"\"\"Segments CT wrt to radial fitted distributions\"\"\"\n",
    "    \"\"\"sample: .h5 file of the CT; scale: the scale of the data i.e 1x, 2x etc; rmin: outer limit for the first chunk of layers\"\"\"\n",
    "    \"\"\"fit: The distribution to segment with; chunk_size: size of CT chunks used for each iteration \"\"\"\n",
    "    tomogram     = h5py.File(seg_path + scale + '_' + sample[:-3]+'_concat.h5', 'r')# path to the h5py file to access data\n",
    "    implant_mask = h5py.File(mask_path + 'Implant-bitmask_'+ scale + '_' + sample, 'r')# path to the h5py file to access data\n",
    "    bone_mask = h5py.File(mask_path + 'Bone-bitmask2_'+ scale + '_' + sample, 'r')# path to the h5py file to access data\n",
    "    print('Files has been loaded')\n",
    "    mask_indices = create_radmask(tomogram['voxels'][0], rmin)# creates mask the CT\n",
    "    print('Mask\\'s made')\n",
    "    n = len(fit)# number of classes\n",
    "    \n",
    "    \n",
    "    # adjusting quirk\n",
    "    shape = implant_mask['mask_voxels'].shape\n",
    "    shape2 = tomogram['voxels'].shape\n",
    "    if shape[0] <= shape2[0]:\n",
    "        shape = shape\n",
    "    else:\n",
    "        shape = shape2\n",
    "    \n",
    "    output, files = create_h5py_files(shape,'rad_corrected', scale, sample, n) # Creates h5py files\n",
    "    print('H5py-files are ready to be written')\n",
    "    for i in tqdm(range(0, shape[0], chunk_size)):\n",
    "        ni = min(chunk_size, shape[0]-i)# takes minimum of the chunk-size and the remaining layers to make sure all layer are included\n",
    "        Layer = tomogram['voxels'][i:i+ni]# loads chunk of the CT\n",
    "        implant = implant_mask['mask_voxels'][i:i+ni]# loads chunk of the bit-mask\n",
    "        bone = bone_mask['mask_voxels'][i:i+ni]# loads chunk of the bit-mask\n",
    "        masked_sample = Layer * implant * bone# applies implant mask to CT chunk\n",
    "        segments = rad_segmentation4(masked_sample, fit, mask_indices, rmin)# segments the masked tomography\n",
    "        for seg in range(n):\n",
    "            output[seg][i:i+ni] = segments[seg]# appends segmented data to compressed h5py file   \n",
    "\n",
    "    for k in range(n): # closes the generate files again\n",
    "        files[k].close()\n",
    "    tomogram.close()\n",
    "    implant_mask.close()\n",
    "    bone_mask.close()\n",
    "    print('Files has been closed')\n",
    "    print('Radial-segmentation is done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 10, 13, 15, 17]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# valid samples: Samples where volume matching worked\n",
    "vs = [0, 2, 4, 6, 10, 13, 15, 17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [07:43<00:00, 27.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# THIS IS WHERE THE WHOLE PROCESS STARTS\n",
    "# 1st Step: Scans sample-data converts into four 2D histograms - radial, z, y and x\n",
    "# Histograms are saved\n",
    "\n",
    "scale = '1x' # scaling \n",
    "N = vs[1]\n",
    "\n",
    "mak1 = h5py.File(mask_path + 'Implant-bitmask_'+ scale + '_' + samples_1x[N], 'r')\n",
    "mak2 = h5py.File(mask_path + 'Bone-bitmask2_'  + scale + '_' + samples_1x[N], 'r')\n",
    "\n",
    "masks = mak1,mak2\n",
    "\n",
    "load_n_save_hists(samples_1x[N], '1x', masks, False, masked = 2, concat = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/868 [00:00<?, ?it/s]/home/madsapp/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/madsapp/.local/lib/python3.6/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 868/868 [03:32<00:00,  4.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The y-histogram has been fitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3rd Step: Fits a pdf to each peak of each layer\n",
    "the_fits = combined_autofitter(data,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4th Step: THe fitting process has some flaws, this step tries to correct them and fill in the lacking information\n",
    "# After testing, this step seems more or less redundant, skip to step 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5th Step: Find ridges of the 2D fits to separate distributions into classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_labels = find_ridges(rad_fits, label_frac = 8, e_iter = 1, d_iter = 20, show = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_labels = find_ridges(z_fits, label_frac = 5, e_iter = 1, d_iter = 10, show = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels   = find_ridges(y_fits, label_frac = 5, e_iter = , d_iter = 10, show = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_labels   = find_ridges(the_fits[3], label_frac = 8, d_iter = 10, show = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving all labels \n",
    "save_labels(rad_labels, 'rad', samples_1x[N],scale, 0)\n",
    "save_labels(z_labels,   'z'  , samples_1x[N],scale, 0)\n",
    "save_labels(y_labels,   'y'  , samples_1x[N],scale, 0)\n",
    "save_labels(x_labels,   'x'  , samples_1x[N],scale, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of array: (5, 433, 254)\n",
      "Shape of array: (4, 805, 254)\n",
      "Shape of array: (5, 868, 254)\n",
      "Shape of array: (5, 868, 254)\n",
      "============================================\n",
      "Size of array 433 - Number of labels 5\n",
      "Size of array 805 - Number of labels 4\n",
      "Size of array 868 - Number of labels 5\n",
      "Size of array 868 - Number of labels 5\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "#Step 6.1: Sorts distributions into classes\n",
    "sorted_rad = distr_sorter(the_fits[0][0], rad_labels)\n",
    "sorted_z =   distr_sorter(the_fits[1][0], z_labels)\n",
    "sorted_y =   distr_sorter(the_fits[2][0], y_labels)\n",
    "sorted_x =   distr_sorter(the_fits[3][0], x_labels)\n",
    "print(11 * '====')\n",
    "#Step 6.2: Sorts parameters into classes\n",
    "sorted_varrad = parameter_sorter(the_fits[0][1], rad_labels)\n",
    "sorted_varz =   parameter_sorter(the_fits[1][1], z_labels)\n",
    "sorted_vary =   parameter_sorter(the_fits[2][1], y_labels)\n",
    "sorted_varx =   parameter_sorter(the_fits[3][1], x_labels)\n",
    "print(11 * '====')\n",
    "# Step 6.3: combines distributions with respective parameters\n",
    "comb_distr  = sorted_rad,    sorted_z,    sorted_y,    sorted_x\n",
    "comb_params = sorted_varrad, sorted_varz, sorted_vary, sorted_varx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves fitted distributions and parameters for later use\n",
    "save_fits_rzyx(comb_distr, comb_params, '1x', samples_1x[N], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If crash because lack of memory - reload fits and decrease chunk_size\n",
    "#sorted_rad, sorted_z, sorted_y, sorted_x = load_fits(samples_1x[N], '1x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 100 # number of layer to chug on at a time\n",
    "scale = '1x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radial segmentation\n",
    "final_rad_segmentation(samples_1x[N], scale, sorted_rad, 20, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z-segmentation\n",
    "final_zyx_segmentation(samples_1x[N], scale, sorted_z, 0, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files has been loaded\n",
      "H5py-files are ready to be written\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:43<00:00,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files has been closed\n",
      "Y-segmentation is done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Y-segmentation\n",
    "final_zyx_segmentation(samples_1x[N], scale, sorted_y, 1, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-segmentation\n",
    "final_zyx_segmentation(samples_1x[N], scale, sorted_x, 2, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are done! yay!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
